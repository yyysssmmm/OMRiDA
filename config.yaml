experiment_name: "dla_baseline"

# ✅ 데이터 경로
data:
  vocab: "data/vocab/all_vocab.txt"
  paired:
    hme_img: "data/paired_CROHME+IM2LATEX/hme"
    pme_img: "data/paired_CROHME+IM2LATEX/pme"
    caption: "data/paired_CROHME+IM2LATEX/caption.txt"
  unpaired:
    pme_img: "data/IM2LATEX/img/pme_unpaired"
    caption: "data/IM2LATEX/caption/unpaired_caption.txt"

# ✅ 모델 하이퍼파라미터
model:
  emb_dim: 64
  encoder_out_channels: 512
  decoder_hidden_dim: 256
  attention_dim: 256

# ✅ 학습 설정
training:
  batch_size: 4
  epochs: 100
  learning_rate: 0.0001
  match_weight: 1.0
  optimizer: "adadelta"         
  grad_clip: 5.0
  early_stop_patience: 5
  ignore_idx: 0

  scheduler:
    use: true
    type: "StepLR"
    step_size: 10
    gamma: 0.5

# ✅ 테스트 설정
testing:
  batch_size: 1
  years: ["2014", "2016", "2019"]
  max_len: 150

# ✅ 기타 설정
misc:
  seed: 42
  device: "mps"   # "cuda" / "cpu"
  checkpoint_path: "runs/dla_baseline_bs8_lr1.0_match1.0_schedStepLR10_0.5_seed42/checkpoints/best_model.pth"
