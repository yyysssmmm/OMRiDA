import torch
import torch.nn as nn
from .encoder import DenseNetEncoder
from .decoder import Decoder
from .DualCrossAttention import DualCrossAttention 

class DLAModel(nn.Module):
    """
    Dual-Cross-Attention ê¸°ë°˜ MER ëª¨ë¸.
    - PME ì´ë¯¸ì§€ì™€ HME ì´ë¯¸ì§€ë¥¼ ê°ê° ì¸ì½”ë”©
    - ì–‘ë°©í–¥ Cross Attention ìˆ˜í–‰
    - PME ë””ì½”ë”ëŠ” HME context ì‚¬ìš©, HME ë””ì½”ë”ëŠ” PME context ì‚¬ìš©
    """

    def __init__(self, vocab_size, model_config=None):  # ğŸ”§ model_config ì¶”ê°€
        super().__init__()

        # ğŸ”§ configì—ì„œ ë°›ì•„ì˜¨ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ê¸°ë³¸ê°’ í¬í•¨)
        emb_dim = model_config.get("emb_dim", 64)
        enc_dim = model_config.get("encoder_out_channels", 512)
        attn_dim = model_config.get("attention_dim", 256)
        hidden_dim = model_config.get("decoder_hidden_dim", 256)

        # ğŸ”§ Encoder, Attention, Decoder êµ¬ì„±
        self.encoder_pme = DenseNetEncoder(out_channels=enc_dim)
        self.encoder_hme = DenseNetEncoder(out_channels=enc_dim)

        self.cross_attention = DualCrossAttention(
            pme_dim=enc_dim, hme_dim=enc_dim, attn_dim=attn_dim
        )

        self.decoder_pme = Decoder(vocab_size, emb_dim, attn_dim, hidden_dim)
        self.decoder_hme = Decoder(vocab_size, emb_dim, attn_dim, hidden_dim)

        self.hidden_dim = hidden_dim  # ë””ì½”ë” ì´ˆê¸° íˆë“ ì— ì‚¬ìš©


    def forward(self, images_pme, images_hme, tgt_seq_pme, tgt_seq_hme, teacher_forcing_ratio=0.5, pme_only=False):
        """
        Args:
            images_pme: (B, 3, H, W)
            images_hme: (B, 3, H, W)
            tgt_seq_pme: (B, T)
            tgt_seq_hme: (B, T)
            pme_only: Trueì´ë©´ HME ì¸ì½”ë”, ë””ì½”ë”, CrossAttentionì„ ìƒëµ (unpaired PME í•™ìŠµì— ì‚¬ìš©)

        Returns:
            logits_pme: (B, T-1, vocab_size)
            logits_hme: (B, T-1, vocab_size) ë˜ëŠ” None
            context_p_to_h: (B, D)
            context_h_to_p: (B, D) ë˜ëŠ” None
        """
        B, T = tgt_seq_pme.shape
        device = images_pme.device
        hidden_pme = torch.zeros(1, B, self.hidden_dim, device=device)  # 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ”ê²Œ ìµœì„ ì¼ê¹Œ?
        hidden_hme = torch.zeros(1, B, self.hidden_dim, device=device)  # 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ”ê²Œ ìµœì„ ì¼ê¹Œ?

        # ğŸ” ì¸ì½”ë”© (PME â†’ Encoder)
        feat_pme = self.encoder_pme(images_pme)  # (B, C, H', W')

        if not pme_only:
            feat_hme = self.encoder_hme(images_hme)  # (B, C, H', W')

            # ğŸ” ì–‘ë°©í–¥ Cross Attention (feat_pme â†” feat_hme)
            context_p_to_h, context_h_to_p, _, _ = self.cross_attention(feat_pme, feat_hme)
        else:
            # ğŸ” ëª¨ë¸ ìˆœì „íŒŒ (unpaired PME â†’ only decoder loss)
            context_p_to_h = torch.zeros(B, self.decoder_pme.attn_dim, device=device)
            context_h_to_p = torch.zeros(B, self.decoder_hme.attn_dim, device=device)

        # ğŸ” ë””ì½”ë”© (teacher forcing)
        outputs_pme = []
        outputs_hme = []

        # ğŸ” PME ë””ì½”ë”©
        prev_token_pme = tgt_seq_pme[:, 0]  # <SOS>
        for t in range(1, tgt_seq_pme.size(1)):
            logits_pme, hidden_pme = self.decoder_pme(prev_token_pme, hidden_pme, context_p_to_h)
            outputs_pme.append(logits_pme.unsqueeze(1)) # (B, 1, V)
            use_tf = torch.rand(1).item() < teacher_forcing_ratio
            if use_tf:
                prev_token_pme = tgt_seq_pme[:, t]
            else:
                prev_token_pme = logits_pme.argmax(dim=-1)

        # ğŸ” HME ë””ì½”ë”©
        if not pme_only:
            prev_token_hme = tgt_seq_hme[:, 0]  # <SOS>
            for t in range(1, tgt_seq_hme.size(1)):
                logits_hme, hidden_hme = self.decoder_hme(prev_token_hme, hidden_hme, context_h_to_p)
                outputs_hme.append(logits_hme.unsqueeze(1))
                use_tf = torch.rand(1).item() < teacher_forcing_ratio
                if use_tf:
                    prev_token_hme = tgt_seq_hme[:, t]
                else:
                    prev_token_hme = logits_hme.argmax(dim=-1)

        logits_pme = torch.cat(outputs_pme, dim=1)  # (B, T, vocab_size)

        if not pme_only:
            logits_hme = torch.cat(outputs_hme, dim=1)
        else:
            logits_hme = None
            context_h_to_p = None

        return logits_pme, logits_hme, context_p_to_h, context_h_to_p


    def predict_hme(self, images_hme, sos_idx, eos_idx, max_len=150):
        """
        HME ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ greedy decodingì„ ìˆ˜í–‰í•˜ëŠ” ì¶”ë¡ ìš© ë©”ì„œë“œ.

        Args:
            images_hme: torch.Tensor (B, 3, H, W) - HME ì´ë¯¸ì§€
            max_len: int - ë””ì½”ë”© ìµœëŒ€ ê¸¸ì´
            sos_idx: int - ì‹œì‘ í† í° ID (<sos>)
            eos_idx: int - ì¢…ë£Œ í† í° ID (<eos>)

        Returns:
            List[List[int]] - ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œë³„ ì˜ˆì¸¡ í† í° ID ì‹œí€€ìŠ¤
        """
        self.eval()
        B = images_hme.size(0)
        device = images_hme.device
        hidden = torch.zeros(1, B, self.hidden_dim, device=device)

        with torch.no_grad():
            # HME ì¸ì½”ë”©
            feat_hme = self.encoder_hme(images_hme)  # (B, C, H', W')

            # Dummy PME feature â†’ CrossAttention ì…ë ¥ìš© (ë‚´ìš©ì€ ìƒê´€ ì—†ìŒ)
            dummy_pme = torch.zeros_like(feat_hme)
            _, context_h_to_p, _, _ = self.cross_attention(dummy_pme, feat_hme)  # (B, D)

            # ë””ì½”ë”© ì‹œì‘
            preds = torch.full((B, 1), sos_idx, dtype=torch.long, device=device)  # ì´ˆê¸° <sos> í† í°

            for _ in range(max_len):
                last_token = preds[:, -1]  # ê°€ì¥ ë§ˆì§€ë§‰ í† í°
                logits, hidden = self.decoder_hme(last_token, hidden, context_h_to_p)  # (B, vocab_size)
                next_token = logits.argmax(dim=-1).unsqueeze(1)  # greedy decoding
                preds = torch.cat([preds, next_token], dim=1)  # ëˆ„ì 

                if (next_token == eos_idx).all():  # ì „ì²´ ë°°ì¹˜ì—ì„œ <eos> ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
                    break

            return preds.tolist()


    def predict_pme(self, images_pme, sos_idx, eos_idx, max_len=150):
        """
        PME ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ greedy decodingì„ ìˆ˜í–‰í•˜ëŠ” ì¶”ë¡ ìš© ë©”ì„œë“œ.

        Args:
            images_pme: torch.Tensor (B, 3, H, W) - PME ì´ë¯¸ì§€
            max_len: int - ë””ì½”ë”© ìµœëŒ€ ê¸¸ì´
            sos_idx: int - ì‹œì‘ í† í° ID (<sos>)
            eos_idx: int - ì¢…ë£Œ í† í° ID (<eos>)

        Returns:
            List[List[int]] - ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œë³„ ì˜ˆì¸¡ í† í° ID ì‹œí€€ìŠ¤
        """
        self.eval()
        B = images_pme.size(0)
        device = images_pme.device
        hidden = torch.zeros(1, B, self.hidden_dim, device=device)

        with torch.no_grad():
            feat_pme = self.encoder_pme(images_pme)
            dummy_hme = torch.zeros_like(feat_pme)
            context_p_to_h, _, _, _ = self.cross_attention(feat_pme, dummy_hme)

            preds = torch.full((B, 1), sos_idx, dtype=torch.long, device=device)

            for _ in range(max_len):
                last_token = preds[:, -1]
                logits, hidden = self.decoder_pme(last_token, hidden, context_p_to_h)
                next_token = logits.argmax(dim=-1).unsqueeze(1)
                preds = torch.cat([preds, next_token], dim=1)

                if (next_token == eos_idx).all():
                    break

            return preds.tolist()

